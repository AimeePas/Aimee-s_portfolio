{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e1c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a24f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Observations (Rows): 12402\n",
      "Number of Variables (Columns): 300\n",
      "\n",
      "Variable Types:\n",
      "Info_PepID           object\n",
      "Info_organism_id      int64\n",
      "Info_protein_id      object\n",
      "Info_pos              int64\n",
      "Info_AA              object\n",
      "                     ...   \n",
      "feat_esm1b_285      float64\n",
      "feat_esm1b_286      float64\n",
      "feat_esm1b_287      float64\n",
      "feat_esm1b_288      float64\n",
      "feat_esm1b_289      float64\n",
      "Length: 300, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/Data_mining_assessment_dataset/df_reduced.csv\", sep = ';')\n",
    "\n",
    "#importing file and define the basic information of the dataframe\n",
    "num_observations = df.shape[0]\n",
    "num_variables = df.shape[1]\n",
    "\n",
    "# Check variable types of the dataframe\n",
    "variable_types = df.dtypes\n",
    "\n",
    "print(\"Number of Observations (Rows):\", num_observations)\n",
    "print(\"Number of Variables (Columns):\", num_variables)\n",
    "print(\"\\nVariable Types:\")\n",
    "print(variable_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350d1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of integer columns: 4\n",
      "Number of float columns: 290\n",
      "Number of string columns: 6\n"
     ]
    }
   ],
   "source": [
    "#counting number of elements present in different datatypes of the dataframe\n",
    "int_count = 0\n",
    "float_count = 0\n",
    "string_count = 0\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'int64':  \n",
    "        int_count += 1\n",
    "    elif df[column].dtype == 'float64': \n",
    "        float_count += 1\n",
    "    elif df[column].dtype == 'object':\n",
    "        string_count +=1\n",
    "print(\"Number of integer columns:\", int_count)\n",
    "print(\"Number of float columns:\", float_count)\n",
    "print(\"Number of string columns:\", string_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f510c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [Info_PepID, Info_organism_id, Info_protein_id, Info_pos, Info_AA, Info_epitope_id, Info_nPos, Info_nNeg, Info_cluster, Class, feat_esm1b_0, feat_esm1b_1, feat_esm1b_2, feat_esm1b_3, feat_esm1b_4, feat_esm1b_5, feat_esm1b_6, feat_esm1b_7, feat_esm1b_8, feat_esm1b_9, feat_esm1b_10, feat_esm1b_11, feat_esm1b_12, feat_esm1b_13, feat_esm1b_14, feat_esm1b_15, feat_esm1b_16, feat_esm1b_17, feat_esm1b_18, feat_esm1b_19, feat_esm1b_20, feat_esm1b_21, feat_esm1b_22, feat_esm1b_23, feat_esm1b_24, feat_esm1b_25, feat_esm1b_26, feat_esm1b_27, feat_esm1b_28, feat_esm1b_29, feat_esm1b_30, feat_esm1b_31, feat_esm1b_32, feat_esm1b_33, feat_esm1b_34, feat_esm1b_35, feat_esm1b_36, feat_esm1b_37, feat_esm1b_38, feat_esm1b_39, feat_esm1b_40, feat_esm1b_41, feat_esm1b_42, feat_esm1b_43, feat_esm1b_44, feat_esm1b_45, feat_esm1b_46, feat_esm1b_47, feat_esm1b_48, feat_esm1b_49, feat_esm1b_50, feat_esm1b_51, feat_esm1b_52, feat_esm1b_53, feat_esm1b_54, feat_esm1b_55, feat_esm1b_56, feat_esm1b_57, feat_esm1b_58, feat_esm1b_59, feat_esm1b_60, feat_esm1b_61, feat_esm1b_62, feat_esm1b_63, feat_esm1b_64, feat_esm1b_65, feat_esm1b_66, feat_esm1b_67, feat_esm1b_68, feat_esm1b_69, feat_esm1b_70, feat_esm1b_71, feat_esm1b_72, feat_esm1b_73, feat_esm1b_74, feat_esm1b_75, feat_esm1b_76, feat_esm1b_77, feat_esm1b_78, feat_esm1b_79, feat_esm1b_80, feat_esm1b_81, feat_esm1b_82, feat_esm1b_83, feat_esm1b_84, feat_esm1b_85, feat_esm1b_86, feat_esm1b_87, feat_esm1b_88, feat_esm1b_89, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "# Print or display the duplicate rows\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea0d01",
   "metadata": {},
   "source": [
    "##### This dataframe does not contain any duplicate values. Neither in information columns nor in features columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b9a264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Info_PepID</th>\n",
       "      <th>Info_organism_id</th>\n",
       "      <th>Info_protein_id</th>\n",
       "      <th>Info_pos</th>\n",
       "      <th>Info_AA</th>\n",
       "      <th>Info_epitope_id</th>\n",
       "      <th>Info_nPos</th>\n",
       "      <th>Info_nNeg</th>\n",
       "      <th>Info_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XP_815234.1:14</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_815234.1</td>\n",
       "      <td>283</td>\n",
       "      <td>S</td>\n",
       "      <td>406709</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XP_811525.1:1</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_811525.1</td>\n",
       "      <td>9</td>\n",
       "      <td>L</td>\n",
       "      <td>339305</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XP_819902.1:4</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_819902.1</td>\n",
       "      <td>96</td>\n",
       "      <td>G</td>\n",
       "      <td>295341</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XP_808204.1:14</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_808204.1</td>\n",
       "      <td>282</td>\n",
       "      <td>Y</td>\n",
       "      <td>315639</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XP_820015.1:10</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_820015.1</td>\n",
       "      <td>242</td>\n",
       "      <td>A</td>\n",
       "      <td>244573,390576</td>\n",
       "      <td>0,0</td>\n",
       "      <td>1,1</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>XP_805059.1:20</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_805059.1</td>\n",
       "      <td>357</td>\n",
       "      <td>P</td>\n",
       "      <td>316771</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12398</th>\n",
       "      <td>XP_804612.1:6</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_804612.1</td>\n",
       "      <td>354</td>\n",
       "      <td>E</td>\n",
       "      <td>408859</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>XP_812323.1:52</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_812323.1</td>\n",
       "      <td>1331</td>\n",
       "      <td>A</td>\n",
       "      <td>343635</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>XP_808278.1:18</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_808278.1</td>\n",
       "      <td>452</td>\n",
       "      <td>Q</td>\n",
       "      <td>413023,323930</td>\n",
       "      <td>0,0</td>\n",
       "      <td>1,1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>XP_819396.1:2</td>\n",
       "      <td>5693</td>\n",
       "      <td>XP_819396.1</td>\n",
       "      <td>66</td>\n",
       "      <td>S</td>\n",
       "      <td>332565</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12402 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Info_PepID  Info_organism_id Info_protein_id  Info_pos Info_AA  \\\n",
       "0      XP_815234.1:14              5693     XP_815234.1       283       S   \n",
       "1       XP_811525.1:1              5693     XP_811525.1         9       L   \n",
       "2       XP_819902.1:4              5693     XP_819902.1        96       G   \n",
       "3      XP_808204.1:14              5693     XP_808204.1       282       Y   \n",
       "4      XP_820015.1:10              5693     XP_820015.1       242       A   \n",
       "...               ...               ...             ...       ...     ...   \n",
       "12397  XP_805059.1:20              5693     XP_805059.1       357       P   \n",
       "12398   XP_804612.1:6              5693     XP_804612.1       354       E   \n",
       "12399  XP_812323.1:52              5693     XP_812323.1      1331       A   \n",
       "12400  XP_808278.1:18              5693     XP_808278.1       452       Q   \n",
       "12401   XP_819396.1:2              5693     XP_819396.1        66       S   \n",
       "\n",
       "      Info_epitope_id Info_nPos Info_nNeg  Info_cluster  \n",
       "0              406709         0         1           188  \n",
       "1              339305         0         1            32  \n",
       "2              295341         0         1            64  \n",
       "3              315639         0         1           102  \n",
       "4       244573,390576       0,0       1,1           211  \n",
       "...               ...       ...       ...           ...  \n",
       "12397          316771         0         1            46  \n",
       "12398          408859         0         1            36  \n",
       "12399          343635         0         1           160  \n",
       "12400   413023,323930       0,0       1,1           103  \n",
       "12401          332565         0         1           247  \n",
       "\n",
       "[12402 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We are dividing the dataframe in information and features because while training the dataset, we do not required information section.\n",
    "#Using iloc, we divide the dataset.\n",
    "#NOTE : the feature dataset contains Class and one column name 'Info_cluster' because we will be using it on later part\n",
    "info_df = df.iloc[:, :9]  \n",
    "feat_df = df.iloc[:, 8:]  \n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c6f081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Info_cluster</th>\n",
       "      <th>Class</th>\n",
       "      <th>feat_esm1b_0</th>\n",
       "      <th>feat_esm1b_1</th>\n",
       "      <th>feat_esm1b_2</th>\n",
       "      <th>feat_esm1b_3</th>\n",
       "      <th>feat_esm1b_4</th>\n",
       "      <th>feat_esm1b_5</th>\n",
       "      <th>feat_esm1b_6</th>\n",
       "      <th>feat_esm1b_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_esm1b_280</th>\n",
       "      <th>feat_esm1b_281</th>\n",
       "      <th>feat_esm1b_282</th>\n",
       "      <th>feat_esm1b_283</th>\n",
       "      <th>feat_esm1b_284</th>\n",
       "      <th>feat_esm1b_285</th>\n",
       "      <th>feat_esm1b_286</th>\n",
       "      <th>feat_esm1b_287</th>\n",
       "      <th>feat_esm1b_288</th>\n",
       "      <th>feat_esm1b_289</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.050457</td>\n",
       "      <td>0.350820</td>\n",
       "      <td>0.081111</td>\n",
       "      <td>0.292722</td>\n",
       "      <td>-0.046843</td>\n",
       "      <td>-0.316361</td>\n",
       "      <td>0.205464</td>\n",
       "      <td>-0.164362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416813</td>\n",
       "      <td>-0.143011</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>-0.317150</td>\n",
       "      <td>0.231682</td>\n",
       "      <td>0.185963</td>\n",
       "      <td>-0.161442</td>\n",
       "      <td>-0.104047</td>\n",
       "      <td>0.207374</td>\n",
       "      <td>0.247171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.200686</td>\n",
       "      <td>-0.200323</td>\n",
       "      <td>-0.367348</td>\n",
       "      <td>-0.286893</td>\n",
       "      <td>-0.289697</td>\n",
       "      <td>-0.061222</td>\n",
       "      <td>-0.441129</td>\n",
       "      <td>0.103984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322140</td>\n",
       "      <td>0.269885</td>\n",
       "      <td>-0.102179</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>-0.145979</td>\n",
       "      <td>-0.010875</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>-0.125649</td>\n",
       "      <td>-0.124625</td>\n",
       "      <td>-0.625465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.089208</td>\n",
       "      <td>0.074643</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.304287</td>\n",
       "      <td>-0.093382</td>\n",
       "      <td>-0.150377</td>\n",
       "      <td>-0.202398</td>\n",
       "      <td>-0.302996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267216</td>\n",
       "      <td>-0.120633</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>-0.533533</td>\n",
       "      <td>-0.077836</td>\n",
       "      <td>-0.115498</td>\n",
       "      <td>-0.234221</td>\n",
       "      <td>-0.578971</td>\n",
       "      <td>-0.195853</td>\n",
       "      <td>0.008974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.150502</td>\n",
       "      <td>0.247198</td>\n",
       "      <td>0.155206</td>\n",
       "      <td>0.029653</td>\n",
       "      <td>0.117658</td>\n",
       "      <td>-0.185720</td>\n",
       "      <td>-0.206774</td>\n",
       "      <td>0.056524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162871</td>\n",
       "      <td>0.244862</td>\n",
       "      <td>-0.060719</td>\n",
       "      <td>0.237254</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.459895</td>\n",
       "      <td>-0.082559</td>\n",
       "      <td>-0.041386</td>\n",
       "      <td>-0.011011</td>\n",
       "      <td>-0.045873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>-0.045094</td>\n",
       "      <td>-0.017004</td>\n",
       "      <td>-0.282002</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>-0.162899</td>\n",
       "      <td>-0.159904</td>\n",
       "      <td>-0.414519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294608</td>\n",
       "      <td>0.092545</td>\n",
       "      <td>-0.244902</td>\n",
       "      <td>-0.222112</td>\n",
       "      <td>-0.148792</td>\n",
       "      <td>0.061015</td>\n",
       "      <td>0.061460</td>\n",
       "      <td>-0.151051</td>\n",
       "      <td>-0.208596</td>\n",
       "      <td>0.079564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.245804</td>\n",
       "      <td>0.466037</td>\n",
       "      <td>0.321277</td>\n",
       "      <td>-0.055060</td>\n",
       "      <td>0.201123</td>\n",
       "      <td>-0.222481</td>\n",
       "      <td>-0.182648</td>\n",
       "      <td>0.022854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010504</td>\n",
       "      <td>0.231793</td>\n",
       "      <td>-0.049883</td>\n",
       "      <td>0.260013</td>\n",
       "      <td>0.066875</td>\n",
       "      <td>-0.362300</td>\n",
       "      <td>-0.468976</td>\n",
       "      <td>0.135828</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>0.365967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12398</th>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.024325</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.114774</td>\n",
       "      <td>0.075034</td>\n",
       "      <td>-0.408322</td>\n",
       "      <td>-0.144037</td>\n",
       "      <td>-0.176111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378880</td>\n",
       "      <td>0.233636</td>\n",
       "      <td>-0.018080</td>\n",
       "      <td>-0.402595</td>\n",
       "      <td>0.277612</td>\n",
       "      <td>-0.098770</td>\n",
       "      <td>-0.243120</td>\n",
       "      <td>-0.286025</td>\n",
       "      <td>-0.277661</td>\n",
       "      <td>-0.145548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>160</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.055652</td>\n",
       "      <td>0.128918</td>\n",
       "      <td>0.062364</td>\n",
       "      <td>0.053716</td>\n",
       "      <td>-0.144050</td>\n",
       "      <td>-0.142040</td>\n",
       "      <td>0.084657</td>\n",
       "      <td>-0.070034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.113434</td>\n",
       "      <td>-0.266736</td>\n",
       "      <td>0.150910</td>\n",
       "      <td>0.415144</td>\n",
       "      <td>-0.289739</td>\n",
       "      <td>0.155924</td>\n",
       "      <td>-0.167771</td>\n",
       "      <td>0.236386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>103</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.193741</td>\n",
       "      <td>0.363251</td>\n",
       "      <td>0.067202</td>\n",
       "      <td>0.200280</td>\n",
       "      <td>-0.168712</td>\n",
       "      <td>-0.037630</td>\n",
       "      <td>-0.051108</td>\n",
       "      <td>0.063174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189327</td>\n",
       "      <td>0.099029</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.276265</td>\n",
       "      <td>-0.282017</td>\n",
       "      <td>0.373733</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.082542</td>\n",
       "      <td>0.168576</td>\n",
       "      <td>0.362109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>247</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.170186</td>\n",
       "      <td>0.185745</td>\n",
       "      <td>0.068464</td>\n",
       "      <td>0.296006</td>\n",
       "      <td>0.293641</td>\n",
       "      <td>-0.390012</td>\n",
       "      <td>0.154005</td>\n",
       "      <td>-0.163800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113387</td>\n",
       "      <td>-0.123360</td>\n",
       "      <td>0.051917</td>\n",
       "      <td>-0.450575</td>\n",
       "      <td>0.276853</td>\n",
       "      <td>0.025367</td>\n",
       "      <td>-0.134704</td>\n",
       "      <td>-0.367695</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>0.209603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12402 rows × 292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Info_cluster  Class  feat_esm1b_0  feat_esm1b_1  feat_esm1b_2  \\\n",
       "0               188     -1     -0.050457      0.350820      0.081111   \n",
       "1                32     -1      0.200686     -0.200323     -0.367348   \n",
       "2                64     -1      0.089208      0.074643      0.005332   \n",
       "3               102     -1      0.150502      0.247198      0.155206   \n",
       "4               211     -1      0.247700     -0.045094     -0.017004   \n",
       "...             ...    ...           ...           ...           ...   \n",
       "12397            46     -1      0.245804      0.466037      0.321277   \n",
       "12398            36     -1      0.006633      0.024325      0.021564   \n",
       "12399           160     -1     -0.055652      0.128918      0.062364   \n",
       "12400           103     -1      0.193741      0.363251      0.067202   \n",
       "12401           247     -1     -0.170186      0.185745      0.068464   \n",
       "\n",
       "       feat_esm1b_3  feat_esm1b_4  feat_esm1b_5  feat_esm1b_6  feat_esm1b_7  \\\n",
       "0          0.292722     -0.046843     -0.316361      0.205464     -0.164362   \n",
       "1         -0.286893     -0.289697     -0.061222     -0.441129      0.103984   \n",
       "2          0.304287     -0.093382     -0.150377     -0.202398     -0.302996   \n",
       "3          0.029653      0.117658     -0.185720     -0.206774      0.056524   \n",
       "4         -0.282002     -0.001418     -0.162899     -0.159904     -0.414519   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "12397     -0.055060      0.201123     -0.222481     -0.182648      0.022854   \n",
       "12398      0.114774      0.075034     -0.408322     -0.144037     -0.176111   \n",
       "12399      0.053716     -0.144050     -0.142040      0.084657     -0.070034   \n",
       "12400      0.200280     -0.168712     -0.037630     -0.051108      0.063174   \n",
       "12401      0.296006      0.293641     -0.390012      0.154005     -0.163800   \n",
       "\n",
       "       ...  feat_esm1b_280  feat_esm1b_281  feat_esm1b_282  feat_esm1b_283  \\\n",
       "0      ...        0.416813       -0.143011        0.022861       -0.317150   \n",
       "1      ...       -0.322140        0.269885       -0.102179        0.325600   \n",
       "2      ...        0.267216       -0.120633       -0.012753       -0.533533   \n",
       "3      ...       -0.162871        0.244862       -0.060719        0.237254   \n",
       "4      ...        0.294608        0.092545       -0.244902       -0.222112   \n",
       "...    ...             ...             ...             ...             ...   \n",
       "12397  ...        0.010504        0.231793       -0.049883        0.260013   \n",
       "12398  ...        0.378880        0.233636       -0.018080       -0.402595   \n",
       "12399  ...        0.022624        0.134560       -0.113434       -0.266736   \n",
       "12400  ...        0.189327        0.099029        0.001823        0.276265   \n",
       "12401  ...        0.113387       -0.123360        0.051917       -0.450575   \n",
       "\n",
       "       feat_esm1b_284  feat_esm1b_285  feat_esm1b_286  feat_esm1b_287  \\\n",
       "0            0.231682        0.185963       -0.161442       -0.104047   \n",
       "1           -0.145979       -0.010875        0.004623       -0.125649   \n",
       "2           -0.077836       -0.115498       -0.234221       -0.578971   \n",
       "3           -0.015220        0.459895       -0.082559       -0.041386   \n",
       "4           -0.148792        0.061015        0.061460       -0.151051   \n",
       "...               ...             ...             ...             ...   \n",
       "12397        0.066875       -0.362300       -0.468976        0.135828   \n",
       "12398        0.277612       -0.098770       -0.243120       -0.286025   \n",
       "12399        0.150910        0.415144       -0.289739        0.155924   \n",
       "12400       -0.282017        0.373733        0.002096        0.082542   \n",
       "12401        0.276853        0.025367       -0.134704       -0.367695   \n",
       "\n",
       "       feat_esm1b_288  feat_esm1b_289  \n",
       "0            0.207374        0.247171  \n",
       "1           -0.124625       -0.625465  \n",
       "2           -0.195853        0.008974  \n",
       "3           -0.011011       -0.045873  \n",
       "4           -0.208596        0.079564  \n",
       "...               ...             ...  \n",
       "12397       -0.016245        0.365967  \n",
       "12398       -0.277661       -0.145548  \n",
       "12399       -0.167771        0.236386  \n",
       "12400        0.168576        0.362109  \n",
       "12401       -0.001120        0.209603  \n",
       "\n",
       "[12402 rows x 292 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b76655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Info_cluster</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12398</th>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>160</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>103</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>247</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12402 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Info_cluster  Class\n",
       "0               188     -1\n",
       "1                32     -1\n",
       "2                64     -1\n",
       "3               102     -1\n",
       "4               211     -1\n",
       "...             ...    ...\n",
       "12397            46     -1\n",
       "12398            36     -1\n",
       "12399           160     -1\n",
       "12400           103     -1\n",
       "12401           247     -1\n",
       "\n",
       "[12402 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we divide the feature and make new dataframe such that it only contains Info_cluster and Class columns.\n",
    "df_clusterclass = feat_df[['Info_cluster','Class']]\n",
    "df_clusterclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523826fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_esm1b_0</th>\n",
       "      <th>feat_esm1b_1</th>\n",
       "      <th>feat_esm1b_2</th>\n",
       "      <th>feat_esm1b_3</th>\n",
       "      <th>feat_esm1b_4</th>\n",
       "      <th>feat_esm1b_5</th>\n",
       "      <th>feat_esm1b_6</th>\n",
       "      <th>feat_esm1b_7</th>\n",
       "      <th>feat_esm1b_8</th>\n",
       "      <th>feat_esm1b_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_esm1b_280</th>\n",
       "      <th>feat_esm1b_281</th>\n",
       "      <th>feat_esm1b_282</th>\n",
       "      <th>feat_esm1b_283</th>\n",
       "      <th>feat_esm1b_284</th>\n",
       "      <th>feat_esm1b_285</th>\n",
       "      <th>feat_esm1b_286</th>\n",
       "      <th>feat_esm1b_287</th>\n",
       "      <th>feat_esm1b_288</th>\n",
       "      <th>feat_esm1b_289</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.050457</td>\n",
       "      <td>0.350820</td>\n",
       "      <td>0.081111</td>\n",
       "      <td>0.292722</td>\n",
       "      <td>-0.046843</td>\n",
       "      <td>-0.316361</td>\n",
       "      <td>0.205464</td>\n",
       "      <td>-0.164362</td>\n",
       "      <td>-0.125390</td>\n",
       "      <td>-0.156784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416813</td>\n",
       "      <td>-0.143011</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>-0.317150</td>\n",
       "      <td>0.231682</td>\n",
       "      <td>0.185963</td>\n",
       "      <td>-0.161442</td>\n",
       "      <td>-0.104047</td>\n",
       "      <td>0.207374</td>\n",
       "      <td>0.247171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200686</td>\n",
       "      <td>-0.200323</td>\n",
       "      <td>-0.367348</td>\n",
       "      <td>-0.286893</td>\n",
       "      <td>-0.289697</td>\n",
       "      <td>-0.061222</td>\n",
       "      <td>-0.441129</td>\n",
       "      <td>0.103984</td>\n",
       "      <td>-0.336376</td>\n",
       "      <td>-0.232301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322140</td>\n",
       "      <td>0.269885</td>\n",
       "      <td>-0.102179</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>-0.145979</td>\n",
       "      <td>-0.010875</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>-0.125649</td>\n",
       "      <td>-0.124625</td>\n",
       "      <td>-0.625465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089208</td>\n",
       "      <td>0.074643</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.304287</td>\n",
       "      <td>-0.093382</td>\n",
       "      <td>-0.150377</td>\n",
       "      <td>-0.202398</td>\n",
       "      <td>-0.302996</td>\n",
       "      <td>-0.405301</td>\n",
       "      <td>-0.347159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267216</td>\n",
       "      <td>-0.120633</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>-0.533533</td>\n",
       "      <td>-0.077836</td>\n",
       "      <td>-0.115498</td>\n",
       "      <td>-0.234221</td>\n",
       "      <td>-0.578971</td>\n",
       "      <td>-0.195853</td>\n",
       "      <td>0.008974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.150502</td>\n",
       "      <td>0.247198</td>\n",
       "      <td>0.155206</td>\n",
       "      <td>0.029653</td>\n",
       "      <td>0.117658</td>\n",
       "      <td>-0.185720</td>\n",
       "      <td>-0.206774</td>\n",
       "      <td>0.056524</td>\n",
       "      <td>-0.077987</td>\n",
       "      <td>0.028505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162871</td>\n",
       "      <td>0.244862</td>\n",
       "      <td>-0.060719</td>\n",
       "      <td>0.237254</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.459895</td>\n",
       "      <td>-0.082559</td>\n",
       "      <td>-0.041386</td>\n",
       "      <td>-0.011011</td>\n",
       "      <td>-0.045873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247700</td>\n",
       "      <td>-0.045094</td>\n",
       "      <td>-0.017004</td>\n",
       "      <td>-0.282002</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>-0.162899</td>\n",
       "      <td>-0.159904</td>\n",
       "      <td>-0.414519</td>\n",
       "      <td>-0.513792</td>\n",
       "      <td>0.041919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294608</td>\n",
       "      <td>0.092545</td>\n",
       "      <td>-0.244902</td>\n",
       "      <td>-0.222112</td>\n",
       "      <td>-0.148792</td>\n",
       "      <td>0.061015</td>\n",
       "      <td>0.061460</td>\n",
       "      <td>-0.151051</td>\n",
       "      <td>-0.208596</td>\n",
       "      <td>0.079564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>0.245804</td>\n",
       "      <td>0.466037</td>\n",
       "      <td>0.321277</td>\n",
       "      <td>-0.055060</td>\n",
       "      <td>0.201123</td>\n",
       "      <td>-0.222481</td>\n",
       "      <td>-0.182648</td>\n",
       "      <td>0.022854</td>\n",
       "      <td>-0.361306</td>\n",
       "      <td>0.055859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010504</td>\n",
       "      <td>0.231793</td>\n",
       "      <td>-0.049883</td>\n",
       "      <td>0.260013</td>\n",
       "      <td>0.066875</td>\n",
       "      <td>-0.362300</td>\n",
       "      <td>-0.468976</td>\n",
       "      <td>0.135828</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>0.365967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12398</th>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.024325</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.114774</td>\n",
       "      <td>0.075034</td>\n",
       "      <td>-0.408322</td>\n",
       "      <td>-0.144037</td>\n",
       "      <td>-0.176111</td>\n",
       "      <td>-0.342193</td>\n",
       "      <td>-0.296429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378880</td>\n",
       "      <td>0.233636</td>\n",
       "      <td>-0.018080</td>\n",
       "      <td>-0.402595</td>\n",
       "      <td>0.277612</td>\n",
       "      <td>-0.098770</td>\n",
       "      <td>-0.243120</td>\n",
       "      <td>-0.286025</td>\n",
       "      <td>-0.277661</td>\n",
       "      <td>-0.145548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>-0.055652</td>\n",
       "      <td>0.128918</td>\n",
       "      <td>0.062364</td>\n",
       "      <td>0.053716</td>\n",
       "      <td>-0.144050</td>\n",
       "      <td>-0.142040</td>\n",
       "      <td>0.084657</td>\n",
       "      <td>-0.070034</td>\n",
       "      <td>-0.256291</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.113434</td>\n",
       "      <td>-0.266736</td>\n",
       "      <td>0.150910</td>\n",
       "      <td>0.415144</td>\n",
       "      <td>-0.289739</td>\n",
       "      <td>0.155924</td>\n",
       "      <td>-0.167771</td>\n",
       "      <td>0.236386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>0.193741</td>\n",
       "      <td>0.363251</td>\n",
       "      <td>0.067202</td>\n",
       "      <td>0.200280</td>\n",
       "      <td>-0.168712</td>\n",
       "      <td>-0.037630</td>\n",
       "      <td>-0.051108</td>\n",
       "      <td>0.063174</td>\n",
       "      <td>0.092369</td>\n",
       "      <td>0.048575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189327</td>\n",
       "      <td>0.099029</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.276265</td>\n",
       "      <td>-0.282017</td>\n",
       "      <td>0.373733</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.082542</td>\n",
       "      <td>0.168576</td>\n",
       "      <td>0.362109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>-0.170186</td>\n",
       "      <td>0.185745</td>\n",
       "      <td>0.068464</td>\n",
       "      <td>0.296006</td>\n",
       "      <td>0.293641</td>\n",
       "      <td>-0.390012</td>\n",
       "      <td>0.154005</td>\n",
       "      <td>-0.163800</td>\n",
       "      <td>-0.181798</td>\n",
       "      <td>-0.107941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113387</td>\n",
       "      <td>-0.123360</td>\n",
       "      <td>0.051917</td>\n",
       "      <td>-0.450575</td>\n",
       "      <td>0.276853</td>\n",
       "      <td>0.025367</td>\n",
       "      <td>-0.134704</td>\n",
       "      <td>-0.367695</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>0.209603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12402 rows × 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat_esm1b_0  feat_esm1b_1  feat_esm1b_2  feat_esm1b_3  feat_esm1b_4  \\\n",
       "0         -0.050457      0.350820      0.081111      0.292722     -0.046843   \n",
       "1          0.200686     -0.200323     -0.367348     -0.286893     -0.289697   \n",
       "2          0.089208      0.074643      0.005332      0.304287     -0.093382   \n",
       "3          0.150502      0.247198      0.155206      0.029653      0.117658   \n",
       "4          0.247700     -0.045094     -0.017004     -0.282002     -0.001418   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "12397      0.245804      0.466037      0.321277     -0.055060      0.201123   \n",
       "12398      0.006633      0.024325      0.021564      0.114774      0.075034   \n",
       "12399     -0.055652      0.128918      0.062364      0.053716     -0.144050   \n",
       "12400      0.193741      0.363251      0.067202      0.200280     -0.168712   \n",
       "12401     -0.170186      0.185745      0.068464      0.296006      0.293641   \n",
       "\n",
       "       feat_esm1b_5  feat_esm1b_6  feat_esm1b_7  feat_esm1b_8  feat_esm1b_9  \\\n",
       "0         -0.316361      0.205464     -0.164362     -0.125390     -0.156784   \n",
       "1         -0.061222     -0.441129      0.103984     -0.336376     -0.232301   \n",
       "2         -0.150377     -0.202398     -0.302996     -0.405301     -0.347159   \n",
       "3         -0.185720     -0.206774      0.056524     -0.077987      0.028505   \n",
       "4         -0.162899     -0.159904     -0.414519     -0.513792      0.041919   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "12397     -0.222481     -0.182648      0.022854     -0.361306      0.055859   \n",
       "12398     -0.408322     -0.144037     -0.176111     -0.342193     -0.296429   \n",
       "12399     -0.142040      0.084657     -0.070034     -0.256291      0.007682   \n",
       "12400     -0.037630     -0.051108      0.063174      0.092369      0.048575   \n",
       "12401     -0.390012      0.154005     -0.163800     -0.181798     -0.107941   \n",
       "\n",
       "       ...  feat_esm1b_280  feat_esm1b_281  feat_esm1b_282  feat_esm1b_283  \\\n",
       "0      ...        0.416813       -0.143011        0.022861       -0.317150   \n",
       "1      ...       -0.322140        0.269885       -0.102179        0.325600   \n",
       "2      ...        0.267216       -0.120633       -0.012753       -0.533533   \n",
       "3      ...       -0.162871        0.244862       -0.060719        0.237254   \n",
       "4      ...        0.294608        0.092545       -0.244902       -0.222112   \n",
       "...    ...             ...             ...             ...             ...   \n",
       "12397  ...        0.010504        0.231793       -0.049883        0.260013   \n",
       "12398  ...        0.378880        0.233636       -0.018080       -0.402595   \n",
       "12399  ...        0.022624        0.134560       -0.113434       -0.266736   \n",
       "12400  ...        0.189327        0.099029        0.001823        0.276265   \n",
       "12401  ...        0.113387       -0.123360        0.051917       -0.450575   \n",
       "\n",
       "       feat_esm1b_284  feat_esm1b_285  feat_esm1b_286  feat_esm1b_287  \\\n",
       "0            0.231682        0.185963       -0.161442       -0.104047   \n",
       "1           -0.145979       -0.010875        0.004623       -0.125649   \n",
       "2           -0.077836       -0.115498       -0.234221       -0.578971   \n",
       "3           -0.015220        0.459895       -0.082559       -0.041386   \n",
       "4           -0.148792        0.061015        0.061460       -0.151051   \n",
       "...               ...             ...             ...             ...   \n",
       "12397        0.066875       -0.362300       -0.468976        0.135828   \n",
       "12398        0.277612       -0.098770       -0.243120       -0.286025   \n",
       "12399        0.150910        0.415144       -0.289739        0.155924   \n",
       "12400       -0.282017        0.373733        0.002096        0.082542   \n",
       "12401        0.276853        0.025367       -0.134704       -0.367695   \n",
       "\n",
       "       feat_esm1b_288  feat_esm1b_289  \n",
       "0            0.207374        0.247171  \n",
       "1           -0.124625       -0.625465  \n",
       "2           -0.195853        0.008974  \n",
       "3           -0.011011       -0.045873  \n",
       "4           -0.208596        0.079564  \n",
       "...               ...             ...  \n",
       "12397       -0.016245        0.365967  \n",
       "12398       -0.277661       -0.145548  \n",
       "12399       -0.167771        0.236386  \n",
       "12400        0.168576        0.362109  \n",
       "12401       -0.001120        0.209603  \n",
       "\n",
       "[12402 rows x 290 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataset purely consist only features and we will be mostly work on this set. \n",
    "feat_only_df  = feat_df.drop(['Info_cluster','Class'],axis=1)\n",
    "feat_only_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de87fd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2481, 292)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are spliting the dataset into training and test set such that 80 percent data will trained and 20 percent data will be for testing\n",
    "# If you see thoroughly, we are stratifying the dataset on the basis of 'Info_cluster'\n",
    "train_feat_df, test_feat_df = train_test_split(feat_df, test_size=0.2,stratify=feat_df['Info_cluster'], random_state=42)\n",
    "test_feat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e3e943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Info_cluster</th>\n",
       "      <th>Class</th>\n",
       "      <th>feat_esm1b_0</th>\n",
       "      <th>feat_esm1b_1</th>\n",
       "      <th>feat_esm1b_2</th>\n",
       "      <th>feat_esm1b_3</th>\n",
       "      <th>feat_esm1b_4</th>\n",
       "      <th>feat_esm1b_5</th>\n",
       "      <th>feat_esm1b_6</th>\n",
       "      <th>feat_esm1b_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_esm1b_280</th>\n",
       "      <th>feat_esm1b_281</th>\n",
       "      <th>feat_esm1b_282</th>\n",
       "      <th>feat_esm1b_283</th>\n",
       "      <th>feat_esm1b_284</th>\n",
       "      <th>feat_esm1b_285</th>\n",
       "      <th>feat_esm1b_286</th>\n",
       "      <th>feat_esm1b_287</th>\n",
       "      <th>feat_esm1b_288</th>\n",
       "      <th>feat_esm1b_289</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>133</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.023255</td>\n",
       "      <td>0.140630</td>\n",
       "      <td>0.259471</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.054747</td>\n",
       "      <td>-0.064916</td>\n",
       "      <td>-0.260543</td>\n",
       "      <td>0.080375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154079</td>\n",
       "      <td>0.253338</td>\n",
       "      <td>-0.186254</td>\n",
       "      <td>0.510995</td>\n",
       "      <td>-0.053482</td>\n",
       "      <td>0.144445</td>\n",
       "      <td>-0.002697</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>-0.320665</td>\n",
       "      <td>0.010839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>168</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.059623</td>\n",
       "      <td>0.081023</td>\n",
       "      <td>0.033723</td>\n",
       "      <td>-0.073021</td>\n",
       "      <td>0.170036</td>\n",
       "      <td>-0.017027</td>\n",
       "      <td>-0.041450</td>\n",
       "      <td>0.200669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240399</td>\n",
       "      <td>0.233510</td>\n",
       "      <td>0.196454</td>\n",
       "      <td>-0.146785</td>\n",
       "      <td>0.254955</td>\n",
       "      <td>0.357822</td>\n",
       "      <td>-0.101534</td>\n",
       "      <td>0.015532</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0.304389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>165</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.045928</td>\n",
       "      <td>0.123546</td>\n",
       "      <td>-0.058769</td>\n",
       "      <td>0.333141</td>\n",
       "      <td>0.179465</td>\n",
       "      <td>-0.323112</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>0.184604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412560</td>\n",
       "      <td>0.258545</td>\n",
       "      <td>0.063443</td>\n",
       "      <td>0.080428</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.227838</td>\n",
       "      <td>0.139499</td>\n",
       "      <td>-0.223463</td>\n",
       "      <td>-0.085601</td>\n",
       "      <td>0.153609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>237</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.198449</td>\n",
       "      <td>0.300430</td>\n",
       "      <td>0.209989</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>-0.135588</td>\n",
       "      <td>-0.289777</td>\n",
       "      <td>-0.186011</td>\n",
       "      <td>0.152740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083302</td>\n",
       "      <td>-0.067410</td>\n",
       "      <td>0.101871</td>\n",
       "      <td>0.172266</td>\n",
       "      <td>0.299253</td>\n",
       "      <td>0.047873</td>\n",
       "      <td>-0.406339</td>\n",
       "      <td>0.034837</td>\n",
       "      <td>0.152048</td>\n",
       "      <td>0.184087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>121</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.168202</td>\n",
       "      <td>0.105676</td>\n",
       "      <td>-0.248348</td>\n",
       "      <td>-0.183606</td>\n",
       "      <td>-0.213992</td>\n",
       "      <td>-0.147158</td>\n",
       "      <td>-0.102100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008836</td>\n",
       "      <td>0.253181</td>\n",
       "      <td>0.038404</td>\n",
       "      <td>-0.038728</td>\n",
       "      <td>0.187455</td>\n",
       "      <td>-0.178443</td>\n",
       "      <td>0.248350</td>\n",
       "      <td>-0.067668</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>0.251896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>124</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.052640</td>\n",
       "      <td>0.467918</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.108707</td>\n",
       "      <td>-0.136490</td>\n",
       "      <td>-0.133409</td>\n",
       "      <td>-0.025271</td>\n",
       "      <td>-0.068689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277964</td>\n",
       "      <td>0.053161</td>\n",
       "      <td>-0.171618</td>\n",
       "      <td>0.130526</td>\n",
       "      <td>0.214888</td>\n",
       "      <td>0.183277</td>\n",
       "      <td>-0.180251</td>\n",
       "      <td>-0.066499</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>0.163642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.063162</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>-0.239400</td>\n",
       "      <td>0.018559</td>\n",
       "      <td>0.052749</td>\n",
       "      <td>-0.331566</td>\n",
       "      <td>0.092595</td>\n",
       "      <td>0.213950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122304</td>\n",
       "      <td>0.108443</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>-0.329146</td>\n",
       "      <td>0.243531</td>\n",
       "      <td>-0.006927</td>\n",
       "      <td>-0.140350</td>\n",
       "      <td>-0.275313</td>\n",
       "      <td>-0.043985</td>\n",
       "      <td>0.212701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>278</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.036675</td>\n",
       "      <td>0.155537</td>\n",
       "      <td>-0.227392</td>\n",
       "      <td>0.449062</td>\n",
       "      <td>0.336923</td>\n",
       "      <td>-0.458683</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.127521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303502</td>\n",
       "      <td>0.325532</td>\n",
       "      <td>-0.168477</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>0.079468</td>\n",
       "      <td>0.088897</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>-0.166317</td>\n",
       "      <td>-0.033279</td>\n",
       "      <td>0.077972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>172</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.046869</td>\n",
       "      <td>0.634133</td>\n",
       "      <td>0.148021</td>\n",
       "      <td>-0.141794</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.123213</td>\n",
       "      <td>-0.078749</td>\n",
       "      <td>0.323970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015774</td>\n",
       "      <td>0.213614</td>\n",
       "      <td>-0.424222</td>\n",
       "      <td>-0.259752</td>\n",
       "      <td>-0.533587</td>\n",
       "      <td>-0.354047</td>\n",
       "      <td>-0.145802</td>\n",
       "      <td>-0.263079</td>\n",
       "      <td>-0.040537</td>\n",
       "      <td>0.345153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.267651</td>\n",
       "      <td>-0.138536</td>\n",
       "      <td>0.178964</td>\n",
       "      <td>0.134914</td>\n",
       "      <td>-0.057202</td>\n",
       "      <td>0.077365</td>\n",
       "      <td>-0.402994</td>\n",
       "      <td>-0.140743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.172469</td>\n",
       "      <td>-0.386982</td>\n",
       "      <td>-0.372984</td>\n",
       "      <td>-0.424886</td>\n",
       "      <td>-0.282443</td>\n",
       "      <td>-0.138744</td>\n",
       "      <td>-0.454544</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>-0.057959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9921 rows × 292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Info_cluster  Class  feat_esm1b_0  feat_esm1b_1  feat_esm1b_2  \\\n",
       "1380           133     -1      0.023255      0.140630      0.259471   \n",
       "3995           168     -1     -0.059623      0.081023      0.033723   \n",
       "8683           165     -1     -0.045928      0.123546     -0.058769   \n",
       "2836           237     -1      0.198449      0.300430      0.209989   \n",
       "6674           121     -1      0.005600      0.168202      0.105676   \n",
       "...            ...    ...           ...           ...           ...   \n",
       "2129           124     -1      0.052640      0.467918      0.033295   \n",
       "1728            14     -1      0.063162      0.084133     -0.239400   \n",
       "1135           278     -1     -0.036675      0.155537     -0.227392   \n",
       "1211           172     -1      0.046869      0.634133      0.148021   \n",
       "6129            50     -1      0.267651     -0.138536      0.178964   \n",
       "\n",
       "      feat_esm1b_3  feat_esm1b_4  feat_esm1b_5  feat_esm1b_6  feat_esm1b_7  \\\n",
       "1380      0.216382      0.054747     -0.064916     -0.260543      0.080375   \n",
       "3995     -0.073021      0.170036     -0.017027     -0.041450      0.200669   \n",
       "8683      0.333141      0.179465     -0.323112      0.055298      0.184604   \n",
       "2836      0.009874     -0.135588     -0.289777     -0.186011      0.152740   \n",
       "6674     -0.248348     -0.183606     -0.213992     -0.147158     -0.102100   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2129      0.108707     -0.136490     -0.133409     -0.025271     -0.068689   \n",
       "1728      0.018559      0.052749     -0.331566      0.092595      0.213950   \n",
       "1135      0.449062      0.336923     -0.458683      0.034574      0.127521   \n",
       "1211     -0.141794      0.002121      0.123213     -0.078749      0.323970   \n",
       "6129      0.134914     -0.057202      0.077365     -0.402994     -0.140743   \n",
       "\n",
       "      ...  feat_esm1b_280  feat_esm1b_281  feat_esm1b_282  feat_esm1b_283  \\\n",
       "1380  ...        0.154079        0.253338       -0.186254        0.510995   \n",
       "3995  ...        0.240399        0.233510        0.196454       -0.146785   \n",
       "8683  ...        0.412560        0.258545        0.063443        0.080428   \n",
       "2836  ...       -0.083302       -0.067410        0.101871        0.172266   \n",
       "6674  ...       -0.008836        0.253181        0.038404       -0.038728   \n",
       "...   ...             ...             ...             ...             ...   \n",
       "2129  ...        0.277964        0.053161       -0.171618        0.130526   \n",
       "1728  ...       -0.122304        0.108443        0.081247       -0.329146   \n",
       "1135  ...        0.303502        0.325532       -0.168477        0.047289   \n",
       "1211  ...        0.015774        0.213614       -0.424222       -0.259752   \n",
       "6129  ...       -0.000125       -0.172469       -0.386982       -0.372984   \n",
       "\n",
       "      feat_esm1b_284  feat_esm1b_285  feat_esm1b_286  feat_esm1b_287  \\\n",
       "1380       -0.053482        0.144445       -0.002697        0.257663   \n",
       "3995        0.254955        0.357822       -0.101534        0.015532   \n",
       "8683        0.001911        0.227838        0.139499       -0.223463   \n",
       "2836        0.299253        0.047873       -0.406339        0.034837   \n",
       "6674        0.187455       -0.178443        0.248350       -0.067668   \n",
       "...              ...             ...             ...             ...   \n",
       "2129        0.214888        0.183277       -0.180251       -0.066499   \n",
       "1728        0.243531       -0.006927       -0.140350       -0.275313   \n",
       "1135        0.079468        0.088897        0.026688       -0.166317   \n",
       "1211       -0.533587       -0.354047       -0.145802       -0.263079   \n",
       "6129       -0.424886       -0.282443       -0.138744       -0.454544   \n",
       "\n",
       "      feat_esm1b_288  feat_esm1b_289  \n",
       "1380       -0.320665        0.010839  \n",
       "3995       -0.026561        0.304389  \n",
       "8683       -0.085601        0.153609  \n",
       "2836        0.152048        0.184087  \n",
       "6674        0.017991        0.251896  \n",
       "...              ...             ...  \n",
       "2129        0.182884        0.163642  \n",
       "1728       -0.043985        0.212701  \n",
       "1135       -0.033279        0.077972  \n",
       "1211       -0.040537        0.345153  \n",
       "6129        0.010595       -0.057959  \n",
       "\n",
       "[9921 rows x 292 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4344f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Info_cluster</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>133</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>168</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>165</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>237</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>121</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>124</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>278</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>172</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9921 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Info_cluster  Class\n",
       "1380           133     -1\n",
       "3995           168     -1\n",
       "8683           165     -1\n",
       "2836           237     -1\n",
       "6674           121     -1\n",
       "...            ...    ...\n",
       "2129           124     -1\n",
       "1728            14     -1\n",
       "1135           278     -1\n",
       "1211           172     -1\n",
       "6129            50     -1\n",
       "\n",
       "[9921 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again we divide the split dataset such that it one contain purely features and other one contains class and info_cluster\n",
    "train_infoclass = train_feat_df[['Info_cluster','Class']]\n",
    "train_infoclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b745506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_esm1b_0</th>\n",
       "      <th>feat_esm1b_1</th>\n",
       "      <th>feat_esm1b_2</th>\n",
       "      <th>feat_esm1b_3</th>\n",
       "      <th>feat_esm1b_4</th>\n",
       "      <th>feat_esm1b_5</th>\n",
       "      <th>feat_esm1b_6</th>\n",
       "      <th>feat_esm1b_7</th>\n",
       "      <th>feat_esm1b_8</th>\n",
       "      <th>feat_esm1b_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_esm1b_280</th>\n",
       "      <th>feat_esm1b_281</th>\n",
       "      <th>feat_esm1b_282</th>\n",
       "      <th>feat_esm1b_283</th>\n",
       "      <th>feat_esm1b_284</th>\n",
       "      <th>feat_esm1b_285</th>\n",
       "      <th>feat_esm1b_286</th>\n",
       "      <th>feat_esm1b_287</th>\n",
       "      <th>feat_esm1b_288</th>\n",
       "      <th>feat_esm1b_289</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>0.023255</td>\n",
       "      <td>0.140630</td>\n",
       "      <td>0.259471</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.054747</td>\n",
       "      <td>-0.064916</td>\n",
       "      <td>-0.260543</td>\n",
       "      <td>0.080375</td>\n",
       "      <td>-0.109487</td>\n",
       "      <td>-0.402776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154079</td>\n",
       "      <td>0.253338</td>\n",
       "      <td>-0.186254</td>\n",
       "      <td>0.510995</td>\n",
       "      <td>-0.053482</td>\n",
       "      <td>0.144445</td>\n",
       "      <td>-0.002697</td>\n",
       "      <td>0.257663</td>\n",
       "      <td>-0.320665</td>\n",
       "      <td>0.010839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>-0.059623</td>\n",
       "      <td>0.081023</td>\n",
       "      <td>0.033723</td>\n",
       "      <td>-0.073021</td>\n",
       "      <td>0.170036</td>\n",
       "      <td>-0.017027</td>\n",
       "      <td>-0.041450</td>\n",
       "      <td>0.200669</td>\n",
       "      <td>-0.144452</td>\n",
       "      <td>-0.386983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240399</td>\n",
       "      <td>0.233510</td>\n",
       "      <td>0.196454</td>\n",
       "      <td>-0.146785</td>\n",
       "      <td>0.254955</td>\n",
       "      <td>0.357822</td>\n",
       "      <td>-0.101534</td>\n",
       "      <td>0.015532</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0.304389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>-0.045928</td>\n",
       "      <td>0.123546</td>\n",
       "      <td>-0.058769</td>\n",
       "      <td>0.333141</td>\n",
       "      <td>0.179465</td>\n",
       "      <td>-0.323112</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>0.184604</td>\n",
       "      <td>-0.067546</td>\n",
       "      <td>-0.297256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412560</td>\n",
       "      <td>0.258545</td>\n",
       "      <td>0.063443</td>\n",
       "      <td>0.080428</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.227838</td>\n",
       "      <td>0.139499</td>\n",
       "      <td>-0.223463</td>\n",
       "      <td>-0.085601</td>\n",
       "      <td>0.153609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>0.198449</td>\n",
       "      <td>0.300430</td>\n",
       "      <td>0.209989</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>-0.135588</td>\n",
       "      <td>-0.289777</td>\n",
       "      <td>-0.186011</td>\n",
       "      <td>0.152740</td>\n",
       "      <td>-0.436255</td>\n",
       "      <td>-0.116423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083302</td>\n",
       "      <td>-0.067410</td>\n",
       "      <td>0.101871</td>\n",
       "      <td>0.172266</td>\n",
       "      <td>0.299253</td>\n",
       "      <td>0.047873</td>\n",
       "      <td>-0.406339</td>\n",
       "      <td>0.034837</td>\n",
       "      <td>0.152048</td>\n",
       "      <td>0.184087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.168202</td>\n",
       "      <td>0.105676</td>\n",
       "      <td>-0.248348</td>\n",
       "      <td>-0.183606</td>\n",
       "      <td>-0.213992</td>\n",
       "      <td>-0.147158</td>\n",
       "      <td>-0.102100</td>\n",
       "      <td>-0.003359</td>\n",
       "      <td>-0.017031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008836</td>\n",
       "      <td>0.253181</td>\n",
       "      <td>0.038404</td>\n",
       "      <td>-0.038728</td>\n",
       "      <td>0.187455</td>\n",
       "      <td>-0.178443</td>\n",
       "      <td>0.248350</td>\n",
       "      <td>-0.067668</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>0.251896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>0.052640</td>\n",
       "      <td>0.467918</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.108707</td>\n",
       "      <td>-0.136490</td>\n",
       "      <td>-0.133409</td>\n",
       "      <td>-0.025271</td>\n",
       "      <td>-0.068689</td>\n",
       "      <td>-0.230588</td>\n",
       "      <td>-0.141884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277964</td>\n",
       "      <td>0.053161</td>\n",
       "      <td>-0.171618</td>\n",
       "      <td>0.130526</td>\n",
       "      <td>0.214888</td>\n",
       "      <td>0.183277</td>\n",
       "      <td>-0.180251</td>\n",
       "      <td>-0.066499</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>0.163642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>0.063162</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>-0.239400</td>\n",
       "      <td>0.018559</td>\n",
       "      <td>0.052749</td>\n",
       "      <td>-0.331566</td>\n",
       "      <td>0.092595</td>\n",
       "      <td>0.213950</td>\n",
       "      <td>-0.360413</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122304</td>\n",
       "      <td>0.108443</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>-0.329146</td>\n",
       "      <td>0.243531</td>\n",
       "      <td>-0.006927</td>\n",
       "      <td>-0.140350</td>\n",
       "      <td>-0.275313</td>\n",
       "      <td>-0.043985</td>\n",
       "      <td>0.212701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>-0.036675</td>\n",
       "      <td>0.155537</td>\n",
       "      <td>-0.227392</td>\n",
       "      <td>0.449062</td>\n",
       "      <td>0.336923</td>\n",
       "      <td>-0.458683</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.127521</td>\n",
       "      <td>-0.073461</td>\n",
       "      <td>-0.194981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303502</td>\n",
       "      <td>0.325532</td>\n",
       "      <td>-0.168477</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>0.079468</td>\n",
       "      <td>0.088897</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>-0.166317</td>\n",
       "      <td>-0.033279</td>\n",
       "      <td>0.077972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>0.046869</td>\n",
       "      <td>0.634133</td>\n",
       "      <td>0.148021</td>\n",
       "      <td>-0.141794</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.123213</td>\n",
       "      <td>-0.078749</td>\n",
       "      <td>0.323970</td>\n",
       "      <td>0.112959</td>\n",
       "      <td>-0.118763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015774</td>\n",
       "      <td>0.213614</td>\n",
       "      <td>-0.424222</td>\n",
       "      <td>-0.259752</td>\n",
       "      <td>-0.533587</td>\n",
       "      <td>-0.354047</td>\n",
       "      <td>-0.145802</td>\n",
       "      <td>-0.263079</td>\n",
       "      <td>-0.040537</td>\n",
       "      <td>0.345153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>0.267651</td>\n",
       "      <td>-0.138536</td>\n",
       "      <td>0.178964</td>\n",
       "      <td>0.134914</td>\n",
       "      <td>-0.057202</td>\n",
       "      <td>0.077365</td>\n",
       "      <td>-0.402994</td>\n",
       "      <td>-0.140743</td>\n",
       "      <td>-0.452656</td>\n",
       "      <td>0.148593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.172469</td>\n",
       "      <td>-0.386982</td>\n",
       "      <td>-0.372984</td>\n",
       "      <td>-0.424886</td>\n",
       "      <td>-0.282443</td>\n",
       "      <td>-0.138744</td>\n",
       "      <td>-0.454544</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>-0.057959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9921 rows × 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat_esm1b_0  feat_esm1b_1  feat_esm1b_2  feat_esm1b_3  feat_esm1b_4  \\\n",
       "1380      0.023255      0.140630      0.259471      0.216382      0.054747   \n",
       "3995     -0.059623      0.081023      0.033723     -0.073021      0.170036   \n",
       "8683     -0.045928      0.123546     -0.058769      0.333141      0.179465   \n",
       "2836      0.198449      0.300430      0.209989      0.009874     -0.135588   \n",
       "6674      0.005600      0.168202      0.105676     -0.248348     -0.183606   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2129      0.052640      0.467918      0.033295      0.108707     -0.136490   \n",
       "1728      0.063162      0.084133     -0.239400      0.018559      0.052749   \n",
       "1135     -0.036675      0.155537     -0.227392      0.449062      0.336923   \n",
       "1211      0.046869      0.634133      0.148021     -0.141794      0.002121   \n",
       "6129      0.267651     -0.138536      0.178964      0.134914     -0.057202   \n",
       "\n",
       "      feat_esm1b_5  feat_esm1b_6  feat_esm1b_7  feat_esm1b_8  feat_esm1b_9  \\\n",
       "1380     -0.064916     -0.260543      0.080375     -0.109487     -0.402776   \n",
       "3995     -0.017027     -0.041450      0.200669     -0.144452     -0.386983   \n",
       "8683     -0.323112      0.055298      0.184604     -0.067546     -0.297256   \n",
       "2836     -0.289777     -0.186011      0.152740     -0.436255     -0.116423   \n",
       "6674     -0.213992     -0.147158     -0.102100     -0.003359     -0.017031   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2129     -0.133409     -0.025271     -0.068689     -0.230588     -0.141884   \n",
       "1728     -0.331566      0.092595      0.213950     -0.360413      0.007564   \n",
       "1135     -0.458683      0.034574      0.127521     -0.073461     -0.194981   \n",
       "1211      0.123213     -0.078749      0.323970      0.112959     -0.118763   \n",
       "6129      0.077365     -0.402994     -0.140743     -0.452656      0.148593   \n",
       "\n",
       "      ...  feat_esm1b_280  feat_esm1b_281  feat_esm1b_282  feat_esm1b_283  \\\n",
       "1380  ...        0.154079        0.253338       -0.186254        0.510995   \n",
       "3995  ...        0.240399        0.233510        0.196454       -0.146785   \n",
       "8683  ...        0.412560        0.258545        0.063443        0.080428   \n",
       "2836  ...       -0.083302       -0.067410        0.101871        0.172266   \n",
       "6674  ...       -0.008836        0.253181        0.038404       -0.038728   \n",
       "...   ...             ...             ...             ...             ...   \n",
       "2129  ...        0.277964        0.053161       -0.171618        0.130526   \n",
       "1728  ...       -0.122304        0.108443        0.081247       -0.329146   \n",
       "1135  ...        0.303502        0.325532       -0.168477        0.047289   \n",
       "1211  ...        0.015774        0.213614       -0.424222       -0.259752   \n",
       "6129  ...       -0.000125       -0.172469       -0.386982       -0.372984   \n",
       "\n",
       "      feat_esm1b_284  feat_esm1b_285  feat_esm1b_286  feat_esm1b_287  \\\n",
       "1380       -0.053482        0.144445       -0.002697        0.257663   \n",
       "3995        0.254955        0.357822       -0.101534        0.015532   \n",
       "8683        0.001911        0.227838        0.139499       -0.223463   \n",
       "2836        0.299253        0.047873       -0.406339        0.034837   \n",
       "6674        0.187455       -0.178443        0.248350       -0.067668   \n",
       "...              ...             ...             ...             ...   \n",
       "2129        0.214888        0.183277       -0.180251       -0.066499   \n",
       "1728        0.243531       -0.006927       -0.140350       -0.275313   \n",
       "1135        0.079468        0.088897        0.026688       -0.166317   \n",
       "1211       -0.533587       -0.354047       -0.145802       -0.263079   \n",
       "6129       -0.424886       -0.282443       -0.138744       -0.454544   \n",
       "\n",
       "      feat_esm1b_288  feat_esm1b_289  \n",
       "1380       -0.320665        0.010839  \n",
       "3995       -0.026561        0.304389  \n",
       "8683       -0.085601        0.153609  \n",
       "2836        0.152048        0.184087  \n",
       "6674        0.017991        0.251896  \n",
       "...              ...             ...  \n",
       "2129        0.182884        0.163642  \n",
       "1728       -0.043985        0.212701  \n",
       "1135       -0.033279        0.077972  \n",
       "1211       -0.040537        0.345153  \n",
       "6129        0.010595       -0.057959  \n",
       "\n",
       "[9921 rows x 290 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat_only_df = train_feat_df.drop(['Info_cluster','Class'],axis=1)\n",
    "train_feat_only_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69105b",
   "metadata": {},
   "source": [
    "### Counting missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "595974f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values: 12165\n"
     ]
    }
   ],
   "source": [
    "missing_values = train_feat_df.isnull().sum().sum()\n",
    "print(\"Total missing values:\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef1c8d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat_esm1b_148    89.628062\n",
       "feat_esm1b_0       0.120956\n",
       "feat_esm1b_178     0.120956\n",
       "feat_esm1b_142     0.120956\n",
       "feat_esm1b_143     0.120956\n",
       "                    ...    \n",
       "feat_esm1b_214     0.090717\n",
       "feat_esm1b_277     0.080637\n",
       "feat_esm1b_270     0.080637\n",
       "feat_esm1b_114     0.080637\n",
       "feat_esm1b_119     0.070557\n",
       "Length: 290, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amount of missing values in the feature's training dataset only and show the percentage of it in descending order.\n",
    "\n",
    "missing_values_per_column = train_feat_only_df.isnull().sum()\n",
    "missing_percentage = train_feat_only_df.isnull().sum()/len(train_feat_only_df)*100\n",
    "missing_percentage.sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afacddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4228230608010177"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_elements = train_feat_only_df.size\n",
    "missing_values = train_feat_only_df.isnull().sum().sum()\n",
    "total_percentage_missing = (missing_values / total_elements) * 100\n",
    "total_percentage_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c72d1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(train_feat_only_df.T.isnull(), aspect='auto', cmap='viridis')\n",
    "plt.title('Matrix Plot of NaN Values')\n",
    "plt.xlabel('Data Samples')\n",
    "plt.ylabel('Features')\n",
    "plt.colorbar(label='NaN Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb0f02",
   "metadata": {},
   "source": [
    "### Variable imbalance. Use of min-max and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f6090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting graph for each feature column's range The blue dot show maximum point in a single column and corresponding orange dot \n",
    "# shows minimum point in the same feature's column.\n",
    "\n",
    "max_values = train_feat_only_df.max()\n",
    "min_values = train_feat_only_df.min()\n",
    "\n",
    "# Plot maxima and minima values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_values, label='Max Values', marker='o')\n",
    "plt.plot(min_values, label='Min Values', marker='x')\n",
    "plt.xlabel('Features Columns')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Maxima and Minima of Numerical Features')\n",
    "plt.legend()\n",
    "plt.margins(x=0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for the range value in the dataframe. \n",
    "ranges = max_values - min_values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(ranges, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Ranges of Numerical Features')\n",
    "plt.grid(True)\n",
    "plt.margins(x=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187da984",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLass balance is like masking the target class such that there will be \n",
    "class_counts = train_infoclass['Class'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Imbalance')\n",
    "plt.margins(x=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21688c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balance = {}\n",
    "\n",
    "for cluster in train_feat_df['Info_cluster'].unique():\n",
    "    cluster_data = train_feat_df[train_feat_df['Info_cluster'] == cluster]\n",
    "    class_balance[cluster] = len(cluster_data)\n",
    "\n",
    "# Step 3: Print class balance under each Info_cluster\n",
    "for cluster, count in class_balance.items():\n",
    "    print(f\"Info_cluster {cluster}: {count} samples\")\n",
    "total_samples = len(train_feat_df)\n",
    "print(f\"\\nTotal samples: {total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92871c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance based on target \n",
    "clusters = list(class_balance.keys())\n",
    "counts = list(class_balance.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(clusters, counts, color='skyblue')\n",
    "plt.title('Class Balance Under Each Info_cluster')\n",
    "plt.xlabel('Info_cluster')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.margins(x=0)\n",
    "plt.xticks(clusters)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_clusters = set()\n",
    "negative_clusters = set()\n",
    "\n",
    "for cluster in train_feat_df['Info_cluster'].unique():\n",
    "    cluster_data = train_feat_df[train_feat_df['Info_cluster'] == cluster]\n",
    "    if (cluster_data > 0).any().any():\n",
    "        positive_clusters.add(cluster)\n",
    "    if (cluster_data < 0).any().any():\n",
    "        negative_clusters.add(cluster)\n",
    "\n",
    "# Step 4: Check overall class balance\n",
    "overall_positive_values = (train_feat_df > 0).any().any()  # Check if any positive values exist in the DataFrame\n",
    "overall_negative_values = (train_feat_df < 0).any().any()  # Check if any negative values exist in the DataFrame\n",
    "\n",
    "print(\"\\nOverall Class Balance:\")\n",
    "print(f\"Positive Values: {overall_positive_values}\")\n",
    "print(f\"Negative Values: {overall_negative_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3edf3e",
   "metadata": {},
   "source": [
    "## Handling Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cc0d7",
   "metadata": {},
   "source": [
    "Using KNN imputer to missing value, we use the method of mean to substitute NaN value. The other method we can use is median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a086e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(train_feat_only_df), columns=train_feat_only_df.columns)\n",
    "\n",
    "n_clusters = 30 \n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "clusters = kmeans.fit_predict(df_imputed)\n",
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = df_imputed.size\n",
    "missing_values = df_imputed.isnull().sum().sum()\n",
    "total_percentage_imputed = (missing_values / total_elements) * 100\n",
    "total_percentage_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd3f860",
   "metadata": {},
   "source": [
    "To verify the missing value, we apply the missing values code again and we observed that there are 0 percent of missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6a616",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d40c4f",
   "metadata": {},
   "source": [
    "outlier is an observation or data point that deviates significantly from other observations in a dataset. It is typically a data point that lies far away from the other data points. Outliers can occur due to various reasons, including measurement error, natural variability in the data, or rare events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_imputed)\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.02)\n",
    "outlier_scores = lof.fit_predict(scaled_data)\n",
    "outliers = df_imputed[outlier_scores == -1]\n",
    "print(\"Identified outliers:\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc119a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) \n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=outlier_scores, cmap='viridis')\n",
    "plt.title('Outlier Visualization (Before Cleaning)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Outlier Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b084ff",
   "metadata": {},
   "source": [
    "There are 290 outlier in the given dataframe, our contamination level is 2 percent and above plot uses PCA technique to plot outlier in such high dimensional dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fd646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ceb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_infoclass.reset_index(drop=True, inplace=True)\n",
    "df_imputed_concat1 = pd.concat([train_infoclass , df_imputed], axis=1)\n",
    "\n",
    "\n",
    "df_imputed_concat1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c90df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices = outlier_scores == -1\n",
    "\n",
    "cleaned_abod_df = df_imputed_concat1[~outlier_indices]\n",
    "print(\"Shape of cleaned DataFrame after removing outliers:\", cleaned_abod_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_abod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data_abod_cleaned = reduced_data[~outlier_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_data_abod_cleaned[:, 0], reduced_data_abod_cleaned[:, 1], c='yellow')\n",
    "plt.title('Outlier Visualization (After Removal)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.colorbar(label='Outlier Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Instantiate MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Step 2: Define columns to scale (exclude 'Info_cluster' and 'Class')\n",
    "columns_to_scale = [col for col in cleaned_abod_df.columns if col not in ['Info_cluster', 'Class']]\n",
    "\n",
    "# Step 3: Fit the scaler to your data and transform it\n",
    "scaled_data = scaler.fit_transform(cleaned_abod_df[columns_to_scale])\n",
    "\n",
    "# Step 4: Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale, index=cleaned_abod_df.index)\n",
    "\n",
    "# Step 5: Merge scaled data with the non-scaled columns\n",
    "merged_df = pd.concat([scaled_df, cleaned_abod_df[['Info_cluster', 'Class']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = scaled_df.max()\n",
    "min_values = scaled_df.min()\n",
    "\n",
    "# Plot maxima and minima values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_values, label='Max Values', marker='o')\n",
    "plt.plot(min_values, label='Min Values', marker='x')\n",
    "plt.xlabel('Features Columns')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Maxima and Minima of Numerical Features')\n",
    "plt.legend()\n",
    "plt.margins(x=0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56a462",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_balance_data = scaler.fit_transform(cleaned_abod_df)\n",
    "\n",
    "# Class Imbalance\n",
    "class_counts = cleaned_abod_df['Class'].value_counts()\n",
    "print(\"Class Distribution:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(cleaned_abod_df, cleaned_abod_df['Class'])\n",
    "print(\"Balanced Class Distribution:\")\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infoclass1 = X_resampled[['Info_cluster', 'Class']]\n",
    "df_infoclass1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting bar graph after balancing the class.\n",
    "class_counts = df_infoclass1['Class'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Imbalance')\n",
    "plt.margins(x=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmerged_df = X_resampled.drop(['Info_cluster','Class'], axis = 1)\n",
    "unmerged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24115c",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Applying pca for reducing our high dimensional dataframe. The number of coloumn changes depending on the number of component we are choosing (eg-0.95) and converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation[1] \\\n",
    "Note: The number of rows will remain same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea40911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_reduct_data = scaler.fit_transform(unmerged_df)\n",
    "\n",
    "pca = PCA(n_components=0.95) \n",
    "pca_result = pca.fit_transform(scaled_reduct_data)\n",
    "\n",
    "# Step 3: Analyze the results\n",
    "print(\"Original shape:\", unmerged_df.shape)\n",
    "print(\"Reduced shape:\", pca_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f46ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA return the value in array form and therefore we required to tranform this into dataframe with coloumn name as before.\n",
    "pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting arrays to dataframe\n",
    "pca_result_df = pd.DataFrame(pca_result, columns=[f\"feat_esm1b_{i}\" for i in range(0, pca_result.shape[1] + 0)])\n",
    "pca_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infoclass1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate the DataFrames again\n",
    "concatenated_df_pca = pd.concat([df_infoclass1, pca_result_df], axis=1)\n",
    "\n",
    "concatenated_df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d906068",
   "metadata": {},
   "source": [
    "### Balancing the class\n",
    "\n",
    "The element in target class that is 1 and -1. The number of element -1 is extremely huge as compare to element 1. Here the balancing is required to reduce the risk of over-fitting. SMOTE helps to balance the class distribution of the dataset, which can improve the performance of machine learning models. SMOTE generates a new synthetic example by linearly interpolating between the feature values of the template sample and the neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fa6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled_reduced, y_resampled_reduced = smote.fit_resample(concatenated_df_pca, concatenated_df_pca['Class'])\n",
    "print(\"Balanced Class Distribution:\")\n",
    "print(pd.Series(y_resampled_reduced).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43190dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_df_pca_infoclass = concatenated_df_pca[['Info_cluster','Class']]\n",
    "concatenate_df_pca_infoclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df_pca_feat = concatenated_df_pca.drop(['Info_cluster','Class'],axis = 1)\n",
    "concatenated_df_pca_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7a350",
   "metadata": {},
   "source": [
    "### Preliminary Testing\n",
    "Here, we are spliting our cleaned data into 80-20 percent and performing the accuracy test using decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Split the data into train and validation sets\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(concatenated_df_pca_feat, concatenated_df_pca['Class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Fit a decision tree classifier on the training data\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X1_train, y1_train)\n",
    "\n",
    "# Step 3: Predict the target variable on the validation set\n",
    "y1_pred = clf.predict(X1_val)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy = accuracy_score(y1_val, y1_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Generate classification report\n",
    "report = classification_report(y1_val, y1_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06007b6b",
   "metadata": {},
   "source": [
    "The accuracy is 98.3 percent for preliminary test. \\\n",
    "Note: If SMOTE is not implemented then it may produce over-fitting result and produces accuracy of 100 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23740582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "group_shuffle_split = GroupShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data into train and test sets using grouped splitting\n",
    "for train_idx, test_idx in group_shuffle_split.split(concatenated_df_pca, concatenated_df_pca['Class'], groups=concatenated_df_pca['Info_cluster']):\n",
    "    X2_train, X2_test = concatenated_df_pca.iloc[train_idx], concatenated_df_pca.iloc[test_idx]\n",
    "    y2_train, y2_test = concatenated_df_pca['Class'].iloc[train_idx], concatenated_df_pca['Class'].iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff37a5b",
   "metadata": {},
   "source": [
    "Group shuffle - Group shuffle splitting is useful when you have data that is grouped or clustered in some way, and you want to ensure that these groups or clusters are preserved in both the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41581a53",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "Hyperparameters are points of choice or configuration that allow a machine learning model to be customized for a specific task or dataset.[2] \\\n",
    "The two of the simplest and most common methods are random search and grid search \\\n",
    "Random Search - Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain \\\n",
    "Grid Search - Define a search space as a grid of hyperparameter values and evaluate every position in the grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c48ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define the classifier and parameter grid\n",
    "clf = DecisionTreeClassifier(random_state=42)  # corrected syntax\n",
    "param_grid = {'max_depth': [3, 5, 7]}\n",
    "\n",
    "# Define GroupKFold cross-validation\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform Grid Search CV\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring=make_scorer(accuracy_score), cv=group_kfold)\n",
    "grid_search.fit(concatenated_df_pca_feat, concatenated_df_pca['Class'], groups=concatenated_df_pca['Info_cluster'])\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_dist = {'max_depth': randint(3, 10)}\n",
    "\n",
    "# Perform Randomized Search CV\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=10, scoring=make_scorer(accuracy_score), cv=group_kfold)\n",
    "random_search.fit(concatenated_df_pca_feat, concatenated_df_pca['Class'], groups=concatenated_df_pca['Info_cluster'])\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c785fb",
   "metadata": {},
   "source": [
    "When We apply grid search with 3 parameter boundary for decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df019e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "best_model = grid_search.best_estimator_  # Change this to your best model\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred = best_model.predict(concatenated_df_pca_feat)\n",
    "\n",
    "# Calculate balanced accuracy on the full dataset (no test set in this case)\n",
    "balanced_acc = balanced_accuracy_score(concatenated_df_pca['Class'], y_pred)\n",
    "\n",
    "print(\"Balanced Accuracy:\", balanced_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d233fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred = best_model.predict(concatenated_df_pca_feat)\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_acc = balanced_accuracy_score(concatenated_df_pca['Class'], y_pred)\n",
    "print(\"Balanced Accuracy:\", balanced_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a7ea1",
   "metadata": {},
   "source": [
    "The balanced accuracy have been calculated for both hyperparameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd827908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define the classifier and parameter grid\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [3, 5, 7]}\n",
    "\n",
    "# Define GroupKFold cross-validation\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform Grid Search CV\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring=make_scorer(accuracy_score), cv=group_kfold)\n",
    "grid_search.fit(concatenated_df_pca_feat, concatenated_df_pca['Class'], groups=concatenated_df_pca['Info_cluster'])\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7afba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GroupKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89419f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target variable\n",
    "X = concatenated_df_pca_feat\n",
    "y = concatenate_df_pca_infoclass['Class']\n",
    "groups = concatenate_df_pca_infoclass['Info_cluster']\n",
    "\n",
    "# Split data into training and testing sets (stratified based on the target variable)\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(X, y, groups, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Claassifiers \n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    #'XGBoost': XGBClassifier(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Evaluating {clf_name}...\")\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "# Calculate cross-validation and calculate balanced accuracy\n",
    "    cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=group_kfold, scoring='balanced_accuracy', groups=groups_train)\n",
    "    mean_balanced_accuracy = np.mean(cv_scores)\n",
    "    print(f\"Mean Balanced Accuracy: {mean_balanced_accuracy:.4f}\")\n",
    "\n",
    " # Model fitting and printing accuracy and balanced accuracy\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Balanced Accuracy: {test_balanced_accuracy:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6ae9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat_df_infoclass = test_feat_df[['Info_cluster' , 'Class']]\n",
    "test_feat_df_infoclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd342e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_feat_df_new = test_feat_df.drop(['Info_cluster' , 'Class'], axis=1)\n",
    "test_feat_df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16a307",
   "metadata": {},
   "source": [
    "#### Testing on the seen dataset which we split in the very begining which is not cleaned and contains features with target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "n_components = pca.n_components_\n",
    "\n",
    "# preprocessing steps we are applying\n",
    "preprocessing_steps = [\n",
    "    ('imputer', KNNImputer(n_neighbors=30)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('MinMax', MinMaxScaler()),\n",
    "    ('pca', PCA(n_components=n_components))  # Dimension reduction\n",
    "]\n",
    "\n",
    "# Define the classifier\n",
    "classifier = DecisionTreeClassifier(**grid_search.best_params_)\n",
    "\n",
    "# Create=ing a preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=preprocessing_steps)\n",
    "\n",
    "# Create a full pipeline combining preprocessing and the classifier\n",
    "pipeline = Pipeline(steps=[('preprocessing', preprocessing_pipeline), ('classifier', classifier)])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(feat_df, info_df['Class'])\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "test_accuracy = pipeline.score(test_feat_df_new, test_feat_df_infoclass['Class'])\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f87021",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = pipeline.predict(test_feat_df_new)\n",
    "balanced_accuracy = balanced_accuracy_score(test_feat_df_infoclass['Class'], predicted_labels)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d771f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd670b9",
   "metadata": {},
   "source": [
    "## Pipeline implementation on df_reduced_holdout dataset\n",
    "This file contain 290 features and other information columns. When we apply pipeline, we need to keep number of features to be same with our trained dataset. This dataset does not contain any target column so hereby we predict on the basis of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44458ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_data = pd.read_csv(\"C:/Users/DELL/Downloads/df_reduced_holdout/df_reduced_holdout.csv\",sep  =\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing our testing into information column and features columns.\n",
    "info_new_df = df_new_data.iloc[:, :9]  \n",
    "feat_new_df = df_new_data.iloc[:, 9:]  \n",
    "feat_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa129d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_new_array = feat_new_df.values\n",
    "\n",
    "# Preprocess holdout data using the pipeline\n",
    "preprocessed_holdout = pipeline['preprocessing'].transform(feat_new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31053635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying pipeline to the new dataframe and generating the csv file such that it contains 3 columns name Info_PepID, Info_pos \n",
    "# and Prediction column which will carry our predicted values.\n",
    "\n",
    "preprocessed_holdout = pipeline['preprocessing'].transform(feat_new_df)\n",
    "\n",
    "# Predict classes for holdout data\n",
    "predictions = pipeline['classifier'].predict(preprocessed_holdout)\n",
    "\n",
    "# Create DataFrame with predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Info_PepID': info_new_df['Info_PepID'],\n",
    "    'Info_pos': info_new_df['Info_pos'],\n",
    "    'Prediction': predictions,\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv(\"Karan_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of 1 and -1 in prediction.\n",
    "class_counts = predictions_df['Prediction'].value_counts()\n",
    "print(\"Count of 1:\", class_counts[1] if 1 in class_counts else 0)\n",
    "print(\"Count of -1:\", class_counts[-1] if -1 in class_counts else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In percentage for better understanding\n",
    "total_predictions = len(predictions_df)\n",
    "\n",
    "percentage_1 = (class_counts[1] / total_predictions) * 100 if 1 in class_counts else 0\n",
    "percentage_minus_1 = (class_counts[-1] / total_predictions) * 100 if -1 in class_counts else 0\n",
    "\n",
    "print(\"Percentage of 1:\", percentage_1)\n",
    "print(\"Percentage of -1:\", percentage_minus_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec925c",
   "metadata": {},
   "source": [
    "As we can observed that, the epitopes in T.Cruzi is approx 1 percent from 5612 samples. So, it is rare to find the such epitopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a165901",
   "metadata": {},
   "source": [
    "## References\n",
    "-https://www.javatpoint.com/principal-component-analysis#:~:text=Principal%20Component%20Analysis%20is%20an,the%20help%20of%20orthogonal%20transformation.\n",
    "-https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919272a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
